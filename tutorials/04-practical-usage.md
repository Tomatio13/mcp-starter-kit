# ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«4: å®Ÿè·µå¿œç”¨ ğŸš€

**æ‰€è¦æ™‚é–“: 30åˆ†**  
**å‰æçŸ¥è­˜: [ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«3](03-data-handling.md)å®Œäº†**

## ğŸ¯ ä»Šå›ã®ç›®æ¨™

- Web APIã¨é€£æºã™ã‚‹ãƒ„ãƒ¼ãƒ«ã‚’ä½œæˆ
- è¤‡æ•°æ©Ÿèƒ½ã‚’çµ„ã¿åˆã‚ã›ãŸè¤‡åˆãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè£…
- å®Ÿç”¨çš„ãªãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯ã‚’å­¦ç¿’
- éåŒæœŸå‡¦ç†ã¨ã‚¨ãƒ©ãƒ¼å‡¦ç†ã‚’ãƒã‚¹ã‚¿ãƒ¼

## ğŸŒ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

ä»Šå›ã¯**ã€Œã‚¹ãƒãƒ¼ãƒˆæƒ…å ±åé›†&åˆ†æã‚·ã‚¹ãƒ†ãƒ ã€**ã‚’ä½œã‚Šã¾ã™ã€‚

### ä¸»è¦æ©Ÿèƒ½
1. **Webæƒ…å ±åé›†**: URLã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
2. **ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ**: æ„Ÿæƒ…åˆ†æãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
3. **ãƒ‡ãƒ¼ã‚¿ä¿å­˜**: åˆ†æçµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
4. **ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ**: åˆ†æçµæœã‚’ãƒ¬ãƒãƒ¼ãƒˆå½¢å¼ã§å‡ºåŠ›

## ğŸ“ ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæº–å‚™

### ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
pip install fastmcp requests beautifulsoup4 textblob matplotlib
```

### ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 
```
smart-analyzer/
â”œâ”€â”€ main.py              # ãƒ¡ã‚¤ãƒ³ã‚µãƒ¼ãƒãƒ¼
â”œâ”€â”€ web_scraper.py       # Webæƒ…å ±åé›†
â”œâ”€â”€ text_analyzer.py     # ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ
â”œâ”€â”€ database.py          # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†
â”œâ”€â”€ report_generator.py  # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
â”œâ”€â”€ config.toml         # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
â””â”€â”€ data/               # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«
    â”œâ”€â”€ analysis.db     # åˆ†æçµæœDB
    â””â”€â”€ reports/        # ç”Ÿæˆãƒ¬ãƒãƒ¼ãƒˆ
```

## ğŸ—ï¸ ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆ

### ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: `database.py`
```python
"""
ã‚¹ãƒãƒ¼ãƒˆåˆ†æã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†
"""
import sqlite3
import json
from datetime import datetime
from typing import Dict, List, Optional

class AnalysisDatabase:
    def __init__(self, db_path: str = "data/analysis.db"):
        self.db_path = db_path
        self.init_database()
    
    def init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
        import os
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # åˆ†æå¯¾è±¡URLãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS urls (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    url TEXT UNIQUE NOT NULL,
                    title TEXT,
                    content TEXT,
                    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    status TEXT DEFAULT 'pending'
                )
            """)
            
            # åˆ†æçµæœãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS analyses (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    url_id INTEGER,
                    sentiment_score REAL,
                    sentiment_label TEXT,
                    keywords TEXT,  -- JSONå½¢å¼
                    word_count INTEGER,
                    analyzed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (url_id) REFERENCES urls(id)
                )
            """)
            
            # ãƒ¬ãƒãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS reports (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    name TEXT NOT NULL,
                    description TEXT,
                    data TEXT,  -- JSONå½¢å¼
                    file_path TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            conn.commit()
    
    def get_connection(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
db = AnalysisDatabase()
```

## ğŸ•·ï¸ ã‚¹ãƒ†ãƒƒãƒ—3: Webæƒ…å ±åé›†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

### ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: `web_scraper.py`
```python
"""
Webæƒ…å ±åé›†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
from typing import Dict, Optional

class WebScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def scrape_url(self, url: str) -> Dict:
        """URLã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
            title = soup.find('title')
            title_text = title.get_text().strip() if title else "No Title"
            
            # æœ¬æ–‡å–å¾—ï¼ˆåŸºæœ¬çš„ãªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰
            for script in soup(["script", "style", "nav", "footer", "header"]):
                script.decompose()
            
            content = soup.get_text()
            content = ' '.join(content.split())  # ä½™åˆ†ãªç©ºç™½ã‚’å‰Šé™¤
            
            return {
                "success": True,
                "url": url,
                "title": title_text,
                "content": content[:5000],  # æœ€åˆã®5000æ–‡å­—
                "content_length": len(content),
                "scraped_at": time.time()
            }
        
        except requests.exceptions.RequestException as e:
            return {
                "success": False,
                "url": url,
                "error": f"HTTP Error: {str(e)}",
                "scraped_at": time.time()
            }
        except Exception as e:
            return {
                "success": False,
                "url": url,
                "error": f"Parse Error: {str(e)}",
                "scraped_at": time.time()
            }
    
    def extract_links(self, url: str, base_url: str = None) -> List[str]:
        """ãƒšãƒ¼ã‚¸å†…ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            base_url = base_url or url
            
            links = []
            for link in soup.find_all('a', href=True):
                href = link['href']
                full_url = urljoin(base_url, href)
                if self._is_valid_url(full_url):
                    links.append(full_url)
            
            return list(set(links))  # é‡è¤‡é™¤å»
        
        except Exception as e:
            return []
    
    def _is_valid_url(self, url: str) -> bool:
        """URLã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯"""
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc])
        except:
            return False

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
scraper = WebScraper()
```

## ğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ†ã‚­ã‚¹ãƒˆåˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

### ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: `text_analyzer.py`
```python
"""
ãƒ†ã‚­ã‚¹ãƒˆåˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""
import re
from collections import Counter
from typing import Dict, List, Tuple
import json

try:
    from textblob import TextBlob
    TEXTBLOB_AVAILABLE = True
except ImportError:
    TEXTBLOB_AVAILABLE = False

class TextAnalyzer:
    def __init__(self):
        self.stop_words = {
            'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from',
            'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the',
            'to', 'was', 'will', 'with', 'the', 'this', 'but', 'they', 'have',
            'had', 'what', 'said', 'each', 'which', 'she', 'do', 'how', 'their',
            'if', 'up', 'out', 'many', 'then', 'them', 'these', 'so', 'some',
            'her', 'would', 'make', 'like', 'into', 'him', 'time', 'two', 'more',
            'go', 'no', 'way', 'could', 'my', 'than', 'first', 'been', 'call',
            'who', 'oil', 'sit', 'now', 'find', 'down', 'day', 'did', 'get',
            'come', 'made', 'may', 'part'
        }
    
    def analyze_sentiment(self, text: str) -> Dict:
        """æ„Ÿæƒ…åˆ†æã‚’å®Ÿè¡Œ"""
        if TEXTBLOB_AVAILABLE:
            blob = TextBlob(text)
            polarity = blob.sentiment.polarity
            subjectivity = blob.sentiment.subjectivity
            
            # ãƒ©ãƒ™ãƒ«åˆ¤å®š
            if polarity > 0.1:
                label = "positive"
            elif polarity < -0.1:
                label = "negative"
            else:
                label = "neutral"
            
            return {
                "score": polarity,
                "label": label,
                "subjectivity": subjectivity,
                "method": "textblob"
            }
        else:
            # ç°¡æ˜“æ„Ÿæƒ…åˆ†æï¼ˆTextBlobãŒä½¿ãˆãªã„å ´åˆï¼‰
            return self._simple_sentiment_analysis(text)
    
    def _simple_sentiment_analysis(self, text: str) -> Dict:
        """ç°¡æ˜“æ„Ÿæƒ…åˆ†æ"""
        positive_words = ['good', 'great', 'excellent', 'amazing', 'wonderful', 
                         'fantastic', 'awesome', 'perfect', 'best', 'love']
        negative_words = ['bad', 'terrible', 'awful', 'horrible', 'worst', 
                         'hate', 'disgusting', 'disappointing', 'poor', 'fail']
        
        text_lower = text.lower()
        positive_count = sum(1 for word in positive_words if word in text_lower)
        negative_count = sum(1 for word in negative_words if word in text_lower)
        
        total = positive_count + negative_count
        if total == 0:
            score = 0
            label = "neutral"
        else:
            score = (positive_count - negative_count) / total
            if score > 0.2:
                label = "positive"
            elif score < -0.2:
                label = "negative"
            else:
                label = "neutral"
        
        return {
            "score": score,
            "label": label,
            "subjectivity": 0.5,
            "method": "simple",
            "positive_count": positive_count,
            "negative_count": negative_count
        }
    
    def extract_keywords(self, text: str, top_n: int = 10) -> List[Dict]:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º"""
        # ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        text = re.sub(r'[^\w\s]', ' ', text.lower())
        words = text.split()
        
        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»ã¨é•·ã•ãƒ•ã‚£ãƒ«ã‚¿
        filtered_words = [
            word for word in words 
            if word not in self.stop_words and len(word) > 2
        ]
        
        # é »åº¦ã‚«ã‚¦ãƒ³ãƒˆ
        word_counts = Counter(filtered_words)
        
        # ä¸Šä½ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å–å¾—
        top_keywords = word_counts.most_common(top_n)
        
        return [
            {"word": word, "count": count, "frequency": count/len(filtered_words)}
            for word, count in top_keywords
        ]
    
    def get_text_statistics(self, text: str) -> Dict:
        """ãƒ†ã‚­ã‚¹ãƒˆçµ±è¨ˆæƒ…å ±"""
        words = text.split()
        sentences = text.split('.')
        
        return {
            "word_count": len(words),
            "sentence_count": len(sentences),
            "character_count": len(text),
            "average_word_length": sum(len(word) for word in words) / len(words) if words else 0,
            "average_sentence_length": len(words) / len(sentences) if sentences else 0
        }
    
    def full_analysis(self, text: str) -> Dict:
        """å®Œå…¨åˆ†æ"""
        sentiment = self.analyze_sentiment(text)
        keywords = self.extract_keywords(text)
        statistics = self.get_text_statistics(text)
        
        return {
            "sentiment": sentiment,
            "keywords": keywords,
            "statistics": statistics,
            "analyzed_at": time.time()
        }

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
analyzer = TextAnalyzer()
```

## ğŸ“ˆ ã‚¹ãƒ†ãƒƒãƒ—5: ãƒ¡ã‚¤ãƒ³MCPã‚µãƒ¼ãƒãƒ¼ä½œæˆ

### ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: `main.py`
```python
"""
ã‚¹ãƒãƒ¼ãƒˆæƒ…å ±åé›†&åˆ†æã‚·ã‚¹ãƒ†ãƒ 
FastMCPã‚µãƒ¼ãƒãƒ¼
"""
from fastmcp import FastMCP
from database import db
from web_scraper import scraper
from text_analyzer import analyzer
import json
import time
from typing import Dict, List

app = FastMCP("Smart Information Analyzer")

@app.tool()
def scrape_and_analyze(url: str) -> Dict:
    """URLã‚’å–å¾—ã—ã¦åˆ†æã™ã‚‹ï¼ˆçµ±åˆå‡¦ç†ï¼‰
    
    Args:
        url: åˆ†æå¯¾è±¡ã®URL
        
    Returns:
        ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¨åˆ†æã®çµæœ
    """
    try:
        # 1. Webæƒ…å ±åé›†
        scrape_result = scraper.scrape_url(url)
        
        if not scrape_result["success"]:
            return {
                "success": False,
                "error": scrape_result["error"],
                "stage": "scraping"
            }
        
        # 2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
        with db.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                INSERT OR REPLACE INTO urls (url, title, content, status)
                VALUES (?, ?, ?, 'scraped')
            """, (url, scrape_result["title"], scrape_result["content"]))
            url_id = cursor.lastrowid
        
        # 3. ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ
        analysis_result = analyzer.full_analysis(scrape_result["content"])
        
        # 4. åˆ†æçµæœã‚’ä¿å­˜
        with db.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                INSERT INTO analyses (url_id, sentiment_score, sentiment_label, 
                                    keywords, word_count)
                VALUES (?, ?, ?, ?, ?)
            """, (
                url_id,
                analysis_result["sentiment"]["score"],
                analysis_result["sentiment"]["label"],
                json.dumps(analysis_result["keywords"]),
                analysis_result["statistics"]["word_count"]
            ))
            analysis_id = cursor.lastrowid
        
        return {
            "success": True,
            "url": url,
            "title": scrape_result["title"],
            "content_length": len(scrape_result["content"]),
            "sentiment": analysis_result["sentiment"],
            "top_keywords": analysis_result["keywords"][:5],
            "statistics": analysis_result["statistics"],
            "url_id": url_id,
            "analysis_id": analysis_id
        }
    
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "stage": "processing"
        }

@app.tool()
def batch_analyze_urls(urls: List[str]) -> Dict:
    """è¤‡æ•°URLã‚’ä¸€æ‹¬åˆ†æ
    
    Args:
        urls: åˆ†æå¯¾è±¡URLã®ãƒªã‚¹ãƒˆ
        
    Returns:
        ä¸€æ‹¬åˆ†æã®çµæœ
    """
    results = []
    successful = 0
    failed = 0
    
    for url in urls:
        result = scrape_and_analyze(url)
        results.append({
            "url": url,
            "success": result["success"],
            "result": result
        })
        
        if result["success"]:
            successful += 1
        else:
            failed += 1
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼ˆ1ç§’å¾…æ©Ÿï¼‰
        time.sleep(1)
    
    return {
        "success": True,
        "total_urls": len(urls),
        "successful": successful,
        "failed": failed,
        "results": results
    }

@app.tool()
def get_analysis_history(limit: int = 10) -> Dict:
    """åˆ†æå±¥æ­´ã‚’å–å¾—
    
    Args:
        limit: å–å¾—ã™ã‚‹ä»¶æ•°
        
    Returns:
        åˆ†æå±¥æ­´
    """
    try:
        with db.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT u.url, u.title, a.sentiment_score, a.sentiment_label,
                       a.word_count, a.analyzed_at
                FROM analyses a
                JOIN urls u ON a.url_id = u.id
                ORDER BY a.analyzed_at DESC
                LIMIT ?
            """, (limit,))
            
            history = [dict(row) for row in cursor.fetchall()]
            
            return {
                "success": True,
                "history": history,
                "count": len(history)
            }
    
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

@app.tool()
def search_by_sentiment(sentiment_label: str) -> Dict:
    """æ„Ÿæƒ…ãƒ©ãƒ™ãƒ«ã§æ¤œç´¢
    
    Args:
        sentiment_label: æ„Ÿæƒ…ãƒ©ãƒ™ãƒ«ï¼ˆpositive/negative/neutralï¼‰
        
    Returns:
        æ¤œç´¢çµæœ
    """
    try:
        with db.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT u.url, u.title, a.sentiment_score, a.sentiment_label,
                       a.word_count, a.analyzed_at
                FROM analyses a
                JOIN urls u ON a.url_id = u.id
                WHERE a.sentiment_label = ?
                ORDER BY a.sentiment_score DESC
            """, (sentiment_label,))
            
            results = [dict(row) for row in cursor.fetchall()]
            
            return {
                "success": True,
                "sentiment_label": sentiment_label,
                "results": results,
                "count": len(results)
            }
    
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

@app.tool()
def get_keyword_analysis(min_frequency: float = 0.01) -> Dict:
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ
    
    Args:
        min_frequency: æœ€å°é »åº¦é–¾å€¤
        
    Returns:
        ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æçµæœ
    """
    try:
        from collections import Counter
        
        with db.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT keywords FROM analyses WHERE keywords IS NOT NULL")
            
            all_keywords = Counter()
            for row in cursor.fetchall():
                keywords = json.loads(row["keywords"])
                for kw in keywords:
                    if kw["frequency"] >= min_frequency:
                        all_keywords[kw["word"]] += kw["count"]
            
            top_keywords = all_keywords.most_common(20)
            
            return {
                "success": True,
                "top_keywords": [
                    {"word": word, "total_count": count}
                    for word, count in top_keywords
                ],
                "analyzed_documents": cursor.rowcount
            }
    
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

@app.tool()
def generate_summary_report() -> Dict:
    """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    
    Returns:
        ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ
    """
    try:
        with db.get_connection() as conn:
            cursor = conn.cursor()
            
            # å…¨ä½“çµ±è¨ˆ
            cursor.execute("SELECT COUNT(*) as total FROM analyses")
            total_analyses = cursor.fetchone()["total"]
            
            # æ„Ÿæƒ…åˆ†å¸ƒ
            cursor.execute("""
                SELECT sentiment_label, COUNT(*) as count
                FROM analyses
                GROUP BY sentiment_label
            """)
            sentiment_dist = {row["sentiment_label"]: row["count"] for row in cursor.fetchall()}
            
            # å¹³å‡æ„Ÿæƒ…ã‚¹ã‚³ã‚¢
            cursor.execute("SELECT AVG(sentiment_score) as avg_score FROM analyses")
            avg_sentiment = cursor.fetchone()["avg_score"] or 0
            
            # æœ€è¿‘ã®åˆ†æ
            cursor.execute("""
                SELECT COUNT(*) as recent_count
                FROM analyses
                WHERE analyzed_at > datetime('now', '-7 days')
            """)
            recent_analyses = cursor.fetchone()["recent_count"]
            
            report = {
                "success": True,
                "summary": {
                    "total_analyses": total_analyses,
                    "sentiment_distribution": sentiment_dist,
                    "average_sentiment": avg_sentiment,
                    "recent_analyses_7days": recent_analyses
                },
                "generated_at": time.time()
            }
            
            # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
            cursor.execute("""
                INSERT INTO reports (name, description, data)
                VALUES (?, ?, ?)
            """, (
                "Summary Report",
                f"Generated summary report for {total_analyses} analyses",
                json.dumps(report)
            ))
            
            return report
    
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

if __name__ == "__main__":
    app.run()
```

## ğŸ§ª ã‚¹ãƒ†ãƒƒãƒ—6: ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ

### åŸºæœ¬ãƒ†ã‚¹ãƒˆ
```bash
# ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
python main.py

# å˜ä¸€URLåˆ†æ
echo '{"jsonrpc": "2.0", "id": 1, "method": "tools/call", "params": {"name": "scrape_and_analyze", "arguments": {"url": "https://example.com"}}}' | python main.py

# åˆ†æå±¥æ­´å–å¾—
echo '{"jsonrpc": "2.0", "id": 2, "method": "tools/call", "params": {"name": "get_analysis_history", "arguments": {}}}' | python main.py

# ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
echo '{"jsonrpc": "2.0", "id": 3, "method": "tools/call", "params": {"name": "generate_summary_report", "arguments": {}}}' | python main.py
```

## ğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—7: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®æ´»ç”¨

### `config.toml`
```toml
[server]
name = "Smart Information Analyzer"
version = "1.0.0"
description = "Webæƒ…å ±åé›†ã¨åˆ†æã‚’è¡Œã†MCPã‚µãƒ¼ãƒãƒ¼"

[scraping]
timeout = 10
user_agent = "SmartAnalyzer/1.0"
rate_limit = 1.0  # seconds between requests
max_content_length = 10000

[analysis]
enable_sentiment = true
enable_keywords = true
max_keywords = 20
min_keyword_frequency = 0.01

[database]
path = "data/analysis.db"
backup_interval = 86400  # seconds (daily)

[reports]
output_dir = "data/reports"
formats = ["json", "html"]
```

## âœ… å®Œæˆåº¦ãƒã‚§ãƒƒã‚¯

ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š

- [ ] URLã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãŒå‹•ä½œã™ã‚‹
- [ ] æ„Ÿæƒ…åˆ†æãŒå®Ÿè¡Œã•ã‚Œã‚‹
- [ ] ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºãŒæ©Ÿèƒ½ã™ã‚‹
- [ ] ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ãŒæ­£å¸¸ã«å‹•ä½œã™ã‚‹
- [ ] åˆ†æå±¥æ­´ãŒå–å¾—ã§ãã‚‹
- [ ] ãƒ¬ãƒãƒ¼ãƒˆç”ŸæˆãŒæ©Ÿèƒ½ã™ã‚‹

## ğŸ¯ å¿œç”¨èª²é¡Œ

### ãƒãƒ£ãƒ¬ãƒ³ã‚¸å•é¡Œ
1. **RSS feedåˆ†æ**: RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã‚“ã§è‡ªå‹•åˆ†æ
2. **ç”»åƒåˆ†æ**: ãƒšãƒ¼ã‚¸å†…ã®ç”»åƒã‚’OCRã§åˆ†æ
3. **ç«¶åˆåˆ†æ**: è¤‡æ•°ã‚µã‚¤ãƒˆã®æ¯”è¼ƒåˆ†æ
4. **ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ**: æ™‚ç³»åˆ—ã§ã®æ„Ÿæƒ…å¤‰åŒ–è¿½è·¡

### å®Ÿè£…ä¾‹ï¼ˆRSSåˆ†æï¼‰
```python
@app.tool()
def analyze_rss_feed(rss_url: str, max_items: int = 10) -> Dict:
    """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’åˆ†æ"""
    import feedparser
    
    feed = feedparser.parse(rss_url)
    results = []
    
    for entry in feed.entries[:max_items]:
        if hasattr(entry, 'link'):
            result = scrape_and_analyze(entry.link)
            result['rss_title'] = entry.title
            result['published'] = getattr(entry, 'published', None)
            results.append(result)
    
    return {
        "success": True,
        "feed_title": feed.feed.title,
        "analyzed_items": len(results),
        "results": results
    }
```

## ğŸ‰ å­¦ç¿’æˆæœ

ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ç¿’å¾—ã—ãŸã‚¹ã‚­ãƒ«ï¼š

- âœ… Web APIé€£æº
- âœ… éåŒæœŸå‡¦ç†
- âœ… ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- âœ… è¤‡åˆãƒ„ãƒ¼ãƒ«è¨­è¨ˆ
- âœ… å®Ÿç”¨çš„ãªãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯
- âœ… ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–ã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

## ğŸ”„ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

å®Ÿè·µå¿œç”¨ã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ãŸã‚‰ã€æœ€å¾Œã«ãƒ‡ãƒ—ãƒ­ã‚¤ã«ã¤ã„ã¦å­¦ã³ã¾ã—ã‚‡ã†ï¼

**[â†’ ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«5: ãƒ‡ãƒ—ãƒ­ã‚¤](05-deployment.md)**

---

ğŸ’¡ **ãƒ’ãƒ³ãƒˆ**: ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã¯å®Ÿéš›ã®ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°èª¿æŸ»ã‚„ç«¶åˆåˆ†æã«æ´»ç”¨ã§ãã¾ã™ï¼ 